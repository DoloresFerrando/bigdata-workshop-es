{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Text Classification Using PySpark, MLlib & Doc2Vec\n",
    "\n",
    "[Reference](https://medium.com/towards-artificial-intelligence/multi-class-text-classification-using-pyspark-mllib-doc2vec-dbfcee5b39f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "MAX_MEMORY = \"4g\"\n",
    "#     config(\"spark.driver.memory\", MAX_MEMORY).\n",
    "\n",
    "spark = SparkSession.builder. \\\n",
    "    appName(\"pyspark-nlp\"). \\\n",
    "    config(\"spark.executor.memory\", MAX_MEMORY). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "https://www.kaggle.com/rmisra/news-category-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load and repartition data\n",
    "CORES = 8\n",
    "df = spark.read.json('/dataset/news/huffingtonpost-news.json').repartition(CORES * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.count()\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "df.agg(f.approx_count_distinct(df.category).alias('distinct_categories')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('category').count().orderBy('count', ascending=False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df.category == 'QUEER VOICES'].select('headline').show(truncate=False)\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "CATEGORIES = {\n",
    "    'ARTS' : ['ARTS', 'ARTS & CULTURE', 'CULTURE & ARTS', 'STYLE & BEAUTY', 'STYLE', 'TASTE', 'FOOD & DRINK', 'TRAVEL', 'ENTERTAINMENT', 'COMEDY'],\n",
    "    'ENVIRONMENT' : ['ENVIRONMENT', 'GREEN'],\n",
    "    'LIFE' : ['PARENTING', 'DIVORCE', 'EDUCATION', 'PARENTS', 'FIFTY', 'COLLEGE', 'WEDDINGS', 'WELLNESS', 'HEALTHY LIVING', 'HOME & LIVING'],\n",
    "    'POLITICS' : ['POLITICS', 'BUSINESS', 'MONEY'],\n",
    "    'TECH' : ['SCIENCE', 'TECH'],\n",
    "    'NEWS' : ['THE WORLDPOST', 'WORLDPOST', 'MEDIA', 'WORLD NEWS', 'IMPACT', 'WEIRD NEWS', 'GOOD NEWS', 'WOMEN', 'QUEER VOICES', 'BLACK VOICES', 'LATINO VOICES']\n",
    "}\n",
    "\n",
    "\n",
    "cats_assigned = {}\n",
    "for new_category, old_categories in CATEGORIES.items():\n",
    "    for old_category in old_categories:\n",
    "        cats_assigned[old_category] = new_category\n",
    "\n",
    "# UDF\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def reassign_categories(cat):\n",
    "    if cat in cats_assigned.keys():\n",
    "        return cats_assigned[cat]\n",
    "    else:\n",
    "        return cat\n",
    "\n",
    "reassign_categories_udf = udf(lambda cat: reassign_categories(cat), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('new_cat', reassign_categories_udf(df.category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('new_cat').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing # of categories\n",
    "# \n",
    "KEEP_CATS = ['POLITICS', 'TECH', 'SPORTS', 'CRIME', 'RELIGION']\n",
    "df = df.filter(f.col('new_cat').isin(KEEP_CATS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "df = df. \\\n",
    "    drop('category'). \\\n",
    "    drop('authors'). \\\n",
    "    drop('link'). \\\n",
    "    drop('date'). \\\n",
    "    withColumn('raw_text', f.concat(df.headline, f.lit('. '), df.short_description)). \\\n",
    "    drop('headline'). \\\n",
    "    drop('short_description'). \\\n",
    "    withColumnRenamed('new_cat', 'category'). \\\n",
    "    cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.parsing.preprocessing as gsp\n",
    "from gensim import utils\n",
    "\n",
    "filters = [\n",
    "           gsp.strip_tags, \n",
    "           gsp.strip_punctuation,\n",
    "           gsp.strip_multiple_whitespaces,\n",
    "           gsp.strip_numeric,\n",
    "           gsp.remove_stopwords, \n",
    "           gsp.strip_short, \n",
    "           gsp.stem_text\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing UDF\n",
    "# https://changhsinlee.com/pyspark-udf/\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def clean_text(x):\n",
    "    s = x.lower()\n",
    "    s = utils.to_unicode(s)\n",
    "    for f in filters:\n",
    "        s = f(s)\n",
    "    return s\n",
    "\n",
    "clean_text_udf = udf(lambda text: clean_text(text), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Field to Split Dataset\n",
    "SEED=42\n",
    "TEST_SIZE=0.3\n",
    "\n",
    "from pyspark.sql.functions import rand, when\n",
    "df = df.withColumn('train', when(rand(seed=SEED) >= TEST_SIZE, True).otherwise(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df.withColumn('text', clean_text_udf(df.raw_text)).drop('raw_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset distribution\n",
    "df_raw.groupBy('train').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing `category` field with `StringIndexer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "str_indexer = StringIndexer(inputCol=\"category\", outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing `text` field with `Word2Vec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "word2vec = Word2Vec(vectorSize=100, minCount=1, inputCol=\"tokens\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[str_indexer, tokenizer, word2vec])\n",
    "model = pipeline.fit(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_w2v = model.transform(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_w2v.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Datasets\n",
    "# Thanks Pedro Ferrari for this trick ;)\n",
    "import pyspark.sql.functions as f\n",
    "df_train = df_w2v.filter(f.col('train') == True)\n",
    "df_test = df_w2v.filter(f.col('train') == False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "rf_classifier_pipeline = Pipeline(stages=[rf_classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rf_model = rf_classifier_pipeline.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rf_model.save('/dataset/news/random_forest.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rf_predictions = rf_model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rf_predictions.select('category', 'label', 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "rf_model_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = rf_model_evaluator.evaluate(rf_predictions)\n",
    "print(\"Accuracy = %g\" % (accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
